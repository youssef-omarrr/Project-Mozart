# Model comparison

| Model       | Type / approach                                | Key techniques attempted                                                                                                                                                                                                                                                                                                                                                                           | Strengths / what was learned                                                                                                                                                                                                                                                        | Outcome                                                                                                                                      |
| ----------- | ---------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| **Model 0** | GPT-2 Medium (causal LM) with LoRA fine-tuning | - Integrated LoRA for parameter-efficient adaptation<br>- Custom loss with penalties for repeated `REST` tokens<br>- Flattened JSON-like MIDI representation with BPM, instruments, notes                                                                                                                                                                                                          | - Hands-on experience extending GPT-2 vocab with domain-specific tokens<br>- Learned tokenizer limitations for symbolic data (splitting tokens like `C4_q` to C, 4, q)<br>- Practiced scaling LoRA to large pretrained models                                                       | ❌ Generation collapsed to mostly `REST`; tokenizer fragmentation and extremely long input lines (>100k chars) made this approach impractical |
| **Model 1** | facebook-BART (seq2seq) with LoRA fine-tuning  | - Built dataset with masked inputs and unmasked targets (up to 50 tokens per line)<br>- Added all musical notes into tokenizer<br>- Custom loss penalizing non-music tokens<br>- Attempted rule-based enforcement via loss                                                                                                                                                                         | - Demonstrated ability to adapt `seq2seq` architectures to symbolic sequence generation<br>- Successfully re-engineered the tokenizer to preserve music tokens<br>- Learned about model “adversarial” behavior against loss rules (ignoring penalties)                              | ❌ Generated mostly non-music tokens; masking strategy too brittle; loss not sufficient to enforce structure                                  |
| **Model 2** | Custom Transformer (from scratch)              | - REMI tokenization (compact, music-aware)<br>- Sinusoidal positional encoding<br>- Causal triangular attention mask<br>- GPT-style embedding scaling<br>- LayerNorm before projection<br>- **Structural constraints** on token order (Bar → Position → Pitch → Velocity → Duration)<br>- Composite loss: cross-entropy + structural loss<br>- Training tricks: label smoothing, gradient clipping | - Designed and implemented full Transformer architecture with PyTorch<br>- Integrated domain knowledge into sequence rules<br>- Stabilized training with architectural/training refinements<br>- Built end-to-end pipeline: tokenize → train → validate → generate → synthesize WAV | ✅ First working system: produces coherent piano pieces; still limited quality but proves pipeline and model design                           |


---