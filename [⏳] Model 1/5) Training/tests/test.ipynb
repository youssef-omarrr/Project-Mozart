{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689465d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter import get_unique_tokens\n",
    "\n",
    "MUSIC_NOTES = get_unique_tokens()\n",
    "MUSIC_TOKEN_IDS = None  # Will be populated during initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f32427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_token_ids(tokenizer):\n",
    "    \"\"\"Convert music tokens to token IDs\"\"\"\n",
    "    ids = []\n",
    "    missing_tokens = []\n",
    "    \n",
    "    # Get the actual vocabulary size that includes added tokens\n",
    "    vocab_size = tokenizer.vocab_size + len(tokenizer.get_added_vocab())\n",
    "    print(f\"\\nTokenizer vocab_size: {tokenizer.vocab_size}, added tokens: {len(tokenizer.get_added_vocab())}, total: {vocab_size}\")\n",
    "    \n",
    "    for note in MUSIC_NOTES:\n",
    "        tid = tokenizer.convert_tokens_to_ids(note)\n",
    "        if tid is None or tid < 0 or tid >= vocab_size:\n",
    "            missing_tokens.append((note, tid))\n",
    "        else:\n",
    "            ids.append(int(tid))\n",
    "    if missing_tokens:\n",
    "        print(f\"[WARN] Some MUSIC_NOTES missing in tokenizer: {missing_tokens[:10]} (showing up to 10), total of {len(missing_tokens)}\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2696764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in trining mode.....\n",
      "Trainable params: 884,736 / 161,671,680 (0.55%)\n"
     ]
    }
   ],
   "source": [
    "from load_model_tokenizer import load_model_and_tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(for_training= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSIC_TOKEN_IDS = get_music_token_ids(tokenizer)\n",
    "\n",
    "MUSIC_TOKEN_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc506b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample MUSIC_NOTES (first 20): ['A1.A2.E3.G3_q', 'A1.A2.G1.G2_1/3', 'A1.A2_0.75', 'A1.A2_1.25', 'A1.A2_1.5', 'A1.A2_1/12', 'A1.A2_1/3', 'A1.A2_2.5', 'A1.A2_2/3', 'A1.A2_3.25', 'A1.A2_4/3', 'A1.A2_5/12', 'A1.A2_5/3', 'A1.A2_e', 'A1.A2_h', 'A1.A2_q', 'A1.A2_s', 'A1.A3.A3.A4_q', 'A1.A3.C#3_s', 'A1.A3_1.75']\n",
      "MUSIC_TOKEN_IDS length: 27812\n",
      "MUSIC_TOKEN_IDS sample (first 50): [50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282, 50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291, 50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300, 50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309, 50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318, 50319, 50320, 50321, 50322, 50323]\n",
      "pad_token_id: 1 unk_token_id: 3\n",
      "mask token id (from '<MASK>'): 50269\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample MUSIC_NOTES (first 20):\", MUSIC_NOTES[:20])\n",
    "print(\"MUSIC_TOKEN_IDS length:\", len(MUSIC_TOKEN_IDS))\n",
    "print(\"MUSIC_TOKEN_IDS sample (first 50):\", MUSIC_TOKEN_IDS[:50])\n",
    "\n",
    "# Verify special token ids\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id, \"unk_token_id:\", getattr(tokenizer, \"unk_token_id\", None))\n",
    "print(\"mask token id (from '<MASK>'):\", tokenizer.convert_tokens_to_ids(\"<MASK>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccf6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_2\n",
    "\n",
    "test_2.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24eaf448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç TESTING SPACE TOKEN DETECTION\n",
      "----------------------------------------\n",
      "'G4_1.5 B-4_e' -> ['G4_1.5', 'ƒ†', 'B-4_e'] -> [76932, 1437, 55117]\n",
      "'G4_1.5  B-4_e' -> ['G4_1.5', 'ƒ†', 'ƒ†', 'B-4_e'] -> [76932, 1437, 1437, 55117]\n",
      "' G4_1.5' -> ['ƒ†', 'G4_1.5'] -> [1437, 76932]\n",
      "'G4_1.5 ' -> ['G4_1.5', 'ƒ†'] -> [76932, 1437]\n",
      "Space variant ' ': ID = 3\n",
      "Space variant 'ƒ†': ID = 1437\n",
      "Space variant '√Ñ': ID = 649\n",
      "Space variant '√Ñ ': ID = 3\n",
      "Space variant '‚ñÅ': ID = 3\n",
      "üß™ TESTING ENHANCED LOSS FUNCTION\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in eval mode.....\n",
      "‚úÖ Found 27812 music token IDs\n",
      "Initialized MusicTokenEnforcementLoss with 27812 valid music token IDs\n",
      "Found space token: 'ƒ†' (ID: 1437)\n",
      "Space token ID: 1437\n",
      "Special token: <|startofpiece|> (ID: 50265)\n",
      "Special token: <|endofpiece|> (ID: 50266)\n",
      "Special token: <TRACKS> (ID: 50267)\n",
      "Special token: <TRACKSEP> (ID: 50268)\n",
      "Special token: <NAME= (ID: 50270)\n",
      "Special token: <BPM= (ID: 50271)\n",
      "Special token: <DURATION_BEATS= (ID: 50272)\n",
      "Special token: <DURATION_MINUTES= (ID: 50273)\n",
      "Found 52 rest tokens\n",
      "Always allowed tokens: 2\n",
      "‚úÖ Initialized loss function\n",
      "\n",
      "üìã TEST CASE 1: Normal music sequence with spaces\n",
      "------------------------------------------------------------\n",
      "Input:  <|startofpiece|><NAME=Test><TRACKS><MASK> <MASK> <MASK><|endofpiece|>\n",
      "Target: <|startofpiece|><NAME=Test><TRACKS>G4_1.5 B-4_e E-4_1.5<|endofpiece|>\n",
      "Expected: Should allow music notes and spaces between them\n",
      "Input tokens: torch.Size([1, 13])\n",
      "Target tokens: torch.Size([1, 13])\n",
      "Found 3 mask positions\n",
      "\n",
      "üîç TESTING LOSS FUNCTION:\n",
      "Updated model vocab size to: 78091\n",
      "‚úÖ Loss computed successfully: 17082.4180\n",
      "   - CE Loss: 11.1268\n",
      "   - Penalty Loss: 17071.2910\n",
      "   - Non-music predictions: 3\n",
      "\n",
      "üéØ ANALYZING PREDICTIONS:\n",
      "Top 5 predictions at mask positions:\n",
      "  Mask position 0:\n",
      "    1. '<|endofpiece|>' (EOS) - prob: 0.453\n",
      "    2. '√óƒ∂' (OTHER) - prob: 0.153\n",
      "    3. 'cript' (OTHER) - prob: 0.135\n",
      "    4. 'F#2.A3.B4.B5.F#4.D5_s' (MUSIC) - prob: 0.134\n",
      "    5. 'D3.A4_1/3' (MUSIC) - prob: 0.124\n",
      "  Mask position 1:\n",
      "    1. 'E5.A4.D5.F#4_1/3' (MUSIC) - prob: 0.327\n",
      "    2. 'ƒ†refunds' (OTHER) - prob: 0.202\n",
      "    3. 'ƒ†calendar' (OTHER) - prob: 0.188\n",
      "    4. 'B5.G#5.D5_e' (MUSIC) - prob: 0.147\n",
      "    5. 'C#5.A4.G4.E4_q' (MUSIC) - prob: 0.136\n",
      "  Mask position 2:\n",
      "    1. 'D3.B4.F#3.D5_s' (MUSIC) - prob: 0.279\n",
      "    2. 'ƒ†organizing' (OTHER) - prob: 0.193\n",
      "    3. 'F3.D3.A2.F2_e' (MUSIC) - prob: 0.180\n",
      "    4. 'ƒ†Butter' (OTHER) - prob: 0.178\n",
      "    5. 'G5.B-4.E-4_0.75' (MUSIC) - prob: 0.170\n",
      "\n",
      "üî§ SPACE TOKEN ANALYSIS:\n",
      "Space token ID: 1437\n",
      "Space token representation: 'ƒ†'\n",
      "  Position 0 ('<s>'): context='start', allow_space=False\n",
      "  Position 1 ('<|startofpiece|>'): context='start', allow_space=False\n",
      "  Position 2 ('<NAME='): context='start', allow_space=False\n",
      "  Position 3 ('Test'): context='end', allow_space=False\n",
      "  Position 4 ('>'): context='end', allow_space=False\n",
      "‚úÖ Test case completed successfully\n",
      "\n",
      "üìã TEST CASE 2: Track separator case\n",
      "------------------------------------------------------------\n",
      "Input:  <|startofpiece|><TRACKS><MASK> <TRACKSEP> StringInstrument_3: <MASK><|endofpiece|>\n",
      "Target: <|startofpiece|><TRACKS>G4_1.5 <TRACKSEP> StringInstrument_3: B4_s<|endofpiece|>\n",
      "Expected: Should handle track separators correctly\n",
      "Input tokens: torch.Size([1, 16])\n",
      "Target tokens: torch.Size([1, 16])\n",
      "Found 2 mask positions\n",
      "\n",
      "üîç TESTING LOSS FUNCTION:\n",
      "‚úÖ Loss computed successfully: 17620.0918\n",
      "   - CE Loss: 11.2225\n",
      "   - Penalty Loss: 17608.8691\n",
      "   - Non-music predictions: 2\n",
      "\n",
      "üéØ ANALYZING PREDICTIONS:\n",
      "Top 5 predictions at mask positions:\n",
      "  Mask position 0:\n",
      "    1. 'G5.E5.B2_s' (MUSIC) - prob: 0.351\n",
      "    2. '<|endofpiece|>' (EOS) - prob: 0.213\n",
      "    3. 'E-5.A5.C5_2/3' (MUSIC) - prob: 0.166\n",
      "    4. 'Civil' (OTHER) - prob: 0.137\n",
      "    5. 'ƒ†Planes' (OTHER) - prob: 0.133\n",
      "  Mask position 1:\n",
      "    1. 'FontSize' (OTHER) - prob: 0.216\n",
      "    2. 'E-4.G#3_0.75' (MUSIC) - prob: 0.214\n",
      "    3. 'ƒ†Tracks' (OTHER) - prob: 0.205\n",
      "    4. 'B-5.B-4.G4_s' (MUSIC) - prob: 0.189\n",
      "    5. 'ƒ†SU' (OTHER) - prob: 0.176\n",
      "\n",
      "üî§ SPACE TOKEN ANALYSIS:\n",
      "Space token ID: 1437\n",
      "Space token representation: 'ƒ†'\n",
      "  Position 0 ('<s>'): context='start', allow_space=False\n",
      "  Position 1 ('<|startofpiece|>'): context='start', allow_space=False\n",
      "  Position 2 ('<TRACKS>'): context='start', allow_space=False\n",
      "  Position 3 ('<MASK>'): context='track_separator', allow_space=False\n",
      "  Position 4 ('ƒ†'): context='track_separator', allow_space=False\n",
      "‚úÖ Test case completed successfully\n",
      "\n",
      "üìã TEST CASE 3: Metadata section\n",
      "------------------------------------------------------------\n",
      "Input:  <|startofpiece|><NAME=<MASK>><BPM=<MASK>><TRACKS>test<|endofpiece|>\n",
      "Target: <|startofpiece|><NAME=Test Song><BPM=150.0><TRACKS>test<|endofpiece|>\n",
      "Expected: Should handle metadata correctly\n",
      "Input tokens: torch.Size([1, 12])\n",
      "Target tokens: torch.Size([1, 15])\n",
      "Found 2 mask positions\n",
      "\n",
      "üîç TESTING LOSS FUNCTION:\n",
      "‚úÖ Loss computed successfully: 500000030720.0000\n",
      "   - CE Loss: 499999997952.0000\n",
      "   - Penalty Loss: 20306.2480\n",
      "   - Non-music predictions: 2\n",
      "\n",
      "üéØ ANALYZING PREDICTIONS:\n",
      "Top 5 predictions at mask positions:\n",
      "  Mask position 0:\n",
      "    1. '<|endofpiece|>' (EOS) - prob: 0.534\n",
      "    2. 'C#5.A4.C#5.D5_s' (MUSIC) - prob: 0.130\n",
      "    3. 'ƒ†grow' (OTHER) - prob: 0.116\n",
      "    4. 'oux' (OTHER) - prob: 0.111\n",
      "    5. 'ƒ†menu' (OTHER) - prob: 0.109\n",
      "  Mask position 1:\n",
      "    1. '<|endofpiece|>' (EOS) - prob: 0.272\n",
      "    2. 'G4.C#5_h' (MUSIC) - prob: 0.213\n",
      "    3. 'E5.B2_2/3' (MUSIC) - prob: 0.213\n",
      "    4. 'E2.F#2.D4_1/3' (MUSIC) - prob: 0.154\n",
      "    5. 'ƒ†graduated' (OTHER) - prob: 0.148\n",
      "\n",
      "üî§ SPACE TOKEN ANALYSIS:\n",
      "Space token ID: 1437\n",
      "Space token representation: 'ƒ†'\n",
      "  Position 0 ('<s>'): context='start', allow_space=False\n",
      "  Position 1 ('<|startofpiece|>'): context='start', allow_space=False\n",
      "  Position 2 ('<NAME='): context='start', allow_space=False\n",
      "  Position 3 ('<MASK>'): context='end', allow_space=False\n",
      "  Position 4 ('>'): context='end', allow_space=False\n",
      "‚úÖ Test case completed successfully\n",
      "\n",
      "================================================================================\n",
      "üéâ LOSS FUNCTION TESTING COMPLETED\n",
      "\n",
      "üìã RECOMMENDATIONS BEFORE TRAINING:\n",
      "1. ‚úÖ If all test cases passed, the loss function is ready\n",
      "2. ‚ö†Ô∏è  If space tokens aren't detected correctly, check tokenizer\n",
      "3. ‚ö†Ô∏è  If context detection seems wrong, review the context logic\n",
      "4. üîß If penalties aren't strong enough, increase non_music_penalty\n",
      "5. üìä Monitor the prediction breakdowns during initial training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test_loss_function import *\n",
    "\n",
    "test_tokenizer_space_detection()\n",
    "test_enhanced_loss_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd445ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in eval mode.....\n",
      "Found 0 music token IDs\n",
      "Tokenizer vocab size: 50265\n",
      "Initialized MusicTokenEnforcementLoss with 0 valid music token IDs\n",
      "Model vocab size: 78092\n",
      "Found space token: 'ƒ†' (ID: 1437)\n",
      "Space token ID: 1437\n",
      "Special token: <|startofpiece|> (ID: 50265)\n",
      "Special token: <|endofpiece|> (ID: 50266)\n",
      "Special token: <TRACKS> (ID: 50267)\n",
      "Special token: <TRACKSEP> (ID: 50268)\n",
      "Special token: <NAME= (ID: 50270)\n",
      "Special token: <BPM= (ID: 50271)\n",
      "Special token: <DURATION_BEATS= (ID: 50272)\n",
      "Special token: <DURATION_MINUTES= (ID: 50273)\n",
      "Special token: <MASK> (ID: 50269)\n",
      "Found 52 rest tokens\n",
      "Always allowed tokens: 2\n",
      "Batch size: 1, Sequence length: 47, Vocab size: 78092\n",
      "================================================================================\n",
      "TESTING LOSS FUNCTION\n",
      "================================================================================\n",
      "Input: <|startofpiece|><NAME=Symphony No. 41 in C, K.551, Jupiter><BPM=150.0><DURATION_BEATS=49962.0><DURATION_MINUTES=46.26><TRACKS><mask> <TRACKSEP> StringInstrument_3: <mask><|endofpiece|>\n",
      "Input IDs: [[0, 50265, 50270, 104, 36935, 6119, 440, 4, 3492, 11, 230, 6, 229, 4, 38749, 6, 21217, 15698, 50271, 6115, 4, 288, 15698, 50272, 26518, 5379, 4, 288, 15698, 50273, 3761, 4, 2481, 15698, 50267, 50264, 1437, 50268, 26602, 1121, 41392, 1215, 246, 35, 50264, 50266, 2]]\n",
      "Mask positions: [[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False]]\n",
      "Labels: [[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 50264, -100, -100, -100, -100, -100, -100, -100, -100, 50264, -100, -100]]\n",
      "Label range: 50264 to 50264\n",
      "Total loss: 1000000126976.0000\n",
      "Loss components: {'ce_loss': 999999995904.0, 'non_music_penalty': 137868.25, 'non_music_predictions': 2}\n",
      "\n",
      "================================================================================\n",
      "TESTING CONTEXT-AWARE MASKING\n",
      "================================================================================\n",
      "\n",
      "Position 0:\n",
      "  Token: '<s>' (ID: 0)\n",
      "  Context: start\n",
      "  Space allowed: False\n",
      "  Total allowed tokens: 3\n",
      "  Sample allowed tokens:\n",
      "    <pad> (ID: 1)\n",
      "    <|startofpiece|> (ID: 50265)\n",
      "    <MASK> (ID: 50269)\n",
      "\n",
      "Position 5:\n",
      "  Token: 'ony' (ID: 6119)\n",
      "  Context: metadata\n",
      "  Space allowed: False\n",
      "  Total allowed tokens: 2\n",
      "  Sample allowed tokens:\n",
      "    <pad> (ID: 1)\n",
      "    <MASK> (ID: 50269)\n",
      "\n",
      "Position 10:\n",
      "  Token: ' C' (ID: 230)\n",
      "  Context: metadata\n",
      "  Space allowed: False\n",
      "  Total allowed tokens: 2\n",
      "  Sample allowed tokens:\n",
      "    <pad> (ID: 1)\n",
      "    <MASK> (ID: 50269)\n",
      "\n",
      "Position 46:\n",
      "  Token: '</s>' (ID: 2)\n",
      "  Context: end\n",
      "  Space allowed: False\n",
      "  Total allowed tokens: 3\n",
      "  Sample allowed tokens:\n",
      "    <pad> (ID: 1)\n",
      "    <|endofpiece|> (ID: 50266)\n",
      "    <MASK> (ID: 50269)\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from test_loss_function_2 import test_loss\n",
    "\n",
    "test_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6128925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in eval mode.....\n",
      "Special tokens:\n",
      "  <MASK>: 50269\n",
      "  <|startofpiece|>: 50265\n",
      "  <|endofpiece|>: 50266\n",
      "  <TRACKS>: 50267\n",
      "  <TRACKSEP>: 50268\n",
      "\n",
      "Music tokens (first 20):\n",
      "  A1.A2.E3.G3_q: 50274\n",
      "  A1.A2.G1.G2_1/3: 50275\n",
      "  A1.A2_0.75: 50276\n",
      "  A1.A2_1.25: 50277\n",
      "  A1.A2_1.5: 50278\n",
      "  A1.A2_1/12: 50279\n",
      "  A1.A2_1/3: 50280\n",
      "  A1.A2_2.5: 50281\n",
      "  A1.A2_2/3: 50282\n",
      "  A1.A2_3.25: 50283\n",
      "  A1.A2_4/3: 50284\n",
      "  A1.A2_5/12: 50285\n",
      "  A1.A2_5/3: 50286\n",
      "  A1.A2_e: 50287\n",
      "  A1.A2_h: 50288\n",
      "  A1.A2_q: 50289\n",
      "  A1.A2_s: 50290\n",
      "  A1.A3.A3.A4_q: 50291\n",
      "  A1.A3.C#3_s: 50292\n",
      "  A1.A3_1.75: 50293\n",
      "\n",
      "Tokenizer vocab_size: 50265\n",
      "Added vocab size: 27826\n",
      "Total vocab size: 78091\n",
      "\n",
      "Test encoding: <MASK> C4_q D4_e\n",
      "Encoded: [0, 50269, 1437, 60485, 1437, 63632, 2]\n",
      "Decoded: <s><MASK> C4_q D4_e</s>\n"
     ]
    }
   ],
   "source": [
    "from diagnose_tokens import test_tokens\n",
    "\n",
    "test_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f058b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
