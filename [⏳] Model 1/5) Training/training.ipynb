{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044dac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in trining mode.....\n",
      "Trainable params: 884,736 / 161,671,680 (0.55%)\n"
     ]
    }
   ],
   "source": [
    "from load_model_tokenizer import load_model_and_tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(for_training= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3456fb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 19252\n",
      "Test samples: 4814\n",
      "\n",
      "Tokenizer vocab_size: 50265, added tokens: 27826, total: 78091\n",
      "Found 27812 music token IDs\n",
      "\n",
      "Initialized MusicTokenEnforcementLoss with 27812 valid music token IDs\n",
      "Found 52 rest tokens and 14 special tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90136e49a940408f9fa1a4c6edfef24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [Train]:   0%|          | 0/9626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUGGING PREDICTIONS:\n",
      "Mask positions found: 3\n",
      "First 10 predictions at mask positions:\n",
      "  0: Pred='</s>' | Label='Rest_s'\n",
      "  1: Pred='128' | Label='E5_s'\n",
      "  2: Pred='128' | Label='D5_s'\n",
      "==================================================\n",
      "Initializing mask with vocab size: 78086\n",
      "Set 27812 music tokens to no penalty\n",
      "Set 14 special tokens to no penalty\n",
      "\n",
      "DEBUG LOGITS at position (0,9):\n",
      "Token 42199 original logit: 4.168\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 10.359\n",
      "  <TRACKS>: -1.853\n",
      "  <|startofpiece|>: -1.853\n",
      "  <TRACKSEP>: -1.853\n",
      "  <|endofpiece|>: -1.853\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.129\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 3.082\n",
      "  <mask>: -0.750\n",
      "  <unk>: -1.060\n",
      "  A1.A2_2/3: -1.380\n",
      "  A1.A2_0.75: -1.380\n",
      "\n",
      "DEBUG LOGITS at position (0,25):\n",
      "Token 42199 original logit: -1.433\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.461\n",
      "  <BPM=: -1.937\n",
      "  <|startofpiece|>: -1.937\n",
      "  <DURATION_MINUTES=: -1.937\n",
      "  <MASK>: -1.937\n",
      "\n",
      "DEBUG LOGITS at position (0,9):\n",
      "Token 42199 original logit: 3.078\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.176\n",
      "  <s>: -0.881\n",
      "  A3.A2_11/12: -1.375\n",
      "  A1_h: -1.375\n",
      "  A1_7/12: -1.375\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.963\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 10.867\n",
      "  <unk>: 0.088\n",
      "  <mask>: -1.096\n",
      "  A1.A2_2/3: -1.222\n",
      "  <DURATION_MINUTES=: -1.222\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 4.934\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.363\n",
      "  <unk>: 0.165\n",
      "  A1.A2_2/3: -0.584\n",
      "  <DURATION_MINUTES=: -0.584\n",
      "  <DURATION_BEATS=: -0.584\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 2.951\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.617\n",
      "  <unk>: 1.042\n",
      "  A2.B-2.A2.B-2.A2_s: 0.447\n",
      "  A2.A3.D6_1/3: 0.447\n",
      "  A1_1.5: 0.447\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 2.201\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 4.730\n",
      "  <unk>: 0.884\n",
      "  B4.G4.D6.E6_s: -0.906\n",
      "  B3.D4.F4_1/3: -0.906\n",
      "  A3.F4.C5_q: -0.906\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.164\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.078\n",
      "  <unk>: 1.504\n",
      "  <mask>: -0.160\n",
      "  A5.A4.D5_e: -0.239\n",
      "  A3.E4.C#5.G4_s: -0.239\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 6.004\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.438\n",
      "  <unk>: 2.148\n",
      "  A1.E2.A2.C#2_0.75: 1.041\n",
      "  A1.C5_1/3: 1.041\n",
      "  A1.C#2.E2.A2_s: 1.041\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 5.980\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.195\n",
      "  G#5.E4.D5_q: 0.179\n",
      "  A5.B-5.C6.D6_1/3: 0.179\n",
      "  G#3.B-3_1/3: 0.179\n",
      "  A2_11/6: 0.179\n",
      "\n",
      "DEBUG LOGITS at position (0,25):\n",
      "Token 42199 original logit: 5.656\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.266\n",
      "  <unk>: 1.794\n",
      "  B-5.G#3_5/12: -0.067\n",
      "  A3.G#4.G#5_1/3: -0.067\n",
      "  A3.F4.C5_q: -0.067\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.775\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.441\n",
      "  <unk>: 0.570\n",
      "  <s>: 0.419\n",
      "  A3.A4.C4_s: -0.262\n",
      "  A2.E3.G3_2/3: -0.262\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.729\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.012\n",
      "  <unk>: 1.622\n",
      "  G#4.F4.D4.B3_q: 0.113\n",
      "  F#5.D5.A4_3.0: 0.113\n",
      "  C2.E2.G2.B-2.G4.C3_s: 0.113\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 0.819\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.648\n",
      "  <unk>: -0.935\n",
      "  <TRACKS>: -1.155\n",
      "  <|endofpiece|>: -1.155\n",
      "  <|startofpiece|>: -1.155\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: -0.751\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 2.627\n",
      "  <unk>: 2.150\n",
      "  A1.A2_3.25: 0.728\n",
      "  A1.A2_2/3: 0.728\n",
      "  A1.A2_2.5: 0.728\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 1.579\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.766\n",
      "  <unk>: -0.687\n",
      "  <mask>: -0.817\n",
      "  A1.A2_2/3: -0.969\n",
      "  A1.A2_2.5: -0.969\n",
      "\n",
      "DEBUG LOGITS at position (0,39):\n",
      "Token 42199 original logit: 4.902\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.422\n",
      "  <mask>: 0.970\n",
      "  A1_1/6: 0.257\n",
      "  A1.F#1_s: 0.257\n",
      "  <DURATION_MINUTES=: 0.257\n",
      "\n",
      "DEBUG LOGITS at position (0,28):\n",
      "Token 42199 original logit: 2.244\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.660\n",
      "  <unk>: 0.119\n",
      "  D6_w: -0.176\n",
      "  C5.D4.B-4.C5_1/3: -0.176\n",
      "  B4.E5.G#5.E5_s: -0.176\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 2.334\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 12.609\n",
      "  <unk>: 3.787\n",
      "  A3.C#3.A2_e: 0.536\n",
      "  A3.A2.G3.G2.F#3.F#2_1/3: 0.536\n",
      "  A3.A2.E3_e: 0.536\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 5.715\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.875\n",
      "  <mask>: -0.535\n",
      "  <DURATION_BEATS=: -0.568\n",
      "  <|endofpiece|>: -0.568\n",
      "  <|startofpiece|>: -0.568\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: -0.051\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 2.451\n",
      "  <TRACKS>: -2.244\n",
      "  <|startofpiece|>: -2.244\n",
      "  <TRACKSEP>: -2.244\n",
      "  <|endofpiece|>: -2.244\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 4.367\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 4.801\n",
      "  <s>: 2.529\n",
      "  <unk>: 1.716\n",
      "  E5.E-4_s: 0.578\n",
      "  <|startofpiece|>: 0.578\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 0.881\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.852\n",
      "  <TRACKS>: -1.187\n",
      "  <|startofpiece|>: -1.187\n",
      "  <TRACKSEP>: -1.187\n",
      "  <|endofpiece|>: -1.187\n",
      "\n",
      "DEBUG LOGITS at position (0,9):\n",
      "Token 42199 original logit: 6.293\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 4.684\n",
      "  <TRACKS>: -1.474\n",
      "  <|startofpiece|>: -1.474\n",
      "  <TRACKSEP>: -1.474\n",
      "  <|endofpiece|>: -1.474\n",
      "\n",
      "DEBUG LOGITS at position (0,9):\n",
      "Token 42199 original logit: 3.984\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.820\n",
      "  <MASK>: -1.314\n",
      "  <|startofpiece|>: -1.314\n",
      "  <BPM=: -1.314\n",
      "  <TRACKSEP>: -1.314\n",
      "\n",
      "DEBUG LOGITS at position (0,32):\n",
      "Token 42199 original logit: 3.793\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.082\n",
      "  <unk>: 0.510\n",
      "  B4.E5.G#5.E5_s: -0.166\n",
      "  A3.F#3_1.25: -0.166\n",
      "  A2.E5.A3.C#6_s: -0.166\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 4.121\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.027\n",
      "  <unk>: 0.857\n",
      "  D5.G5.B3_s: -0.235\n",
      "  C#2_3.5: -0.235\n",
      "  A4.C#4.E3.C#4.E3_s: -0.235\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 6.602\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.629\n",
      "  <unk>: 2.461\n",
      "  <mask>: 2.016\n",
      "  A2_11/6: 0.268\n",
      "  A1.A2_2/3: 0.268\n",
      "\n",
      "DEBUG LOGITS at position (0,24):\n",
      "Token 42199 original logit: 5.320\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.895\n",
      "  <unk>: 4.074\n",
      "  <mask>: 2.617\n",
      "  <s>: 1.530\n",
      "  A2_11/6: 1.165\n",
      "\n",
      "DEBUG LOGITS at position (0,25):\n",
      "Token 42199 original logit: 1.184\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.746\n",
      "  <s>: 1.347\n",
      "  <unk>: 0.496\n",
      "  C4.F#3.E-3_e: -0.186\n",
      "  A5.F5.A4_s: -0.186\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.660\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.660\n",
      "  <unk>: 0.536\n",
      "  A3.A3_h: -0.421\n",
      "  A2.E4.A3.A3.A4_s: -0.421\n",
      "  A2.A3.E5.A5.C#6_0.75: -0.421\n",
      "\n",
      "DEBUG LOGITS at position (0,28):\n",
      "Token 42199 original logit: 0.343\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.051\n",
      "  <unk>: 0.197\n",
      "  A3.E-5_e: -0.438\n",
      "  A3.C4.F#3.D3_s: -0.438\n",
      "  A3.A4.C#4_e: -0.438\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 4.328\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.129\n",
      "  B2.G5.F#5_s: 0.375\n",
      "  A2.G3.C#4.E3_e: 0.375\n",
      "  A2.A3_0.75: 0.375\n",
      "  A2.A3.A5.A4_e: 0.375\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 1.027\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 4.137\n",
      "  <unk>: -0.320\n",
      "  A2.A1.B2.B1.C#3.C#2_1/3: -1.241\n",
      "  A2.A1.A1.A2_s: -1.241\n",
      "  A1.A2_0.75: -1.241\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 4.746\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 8.609\n",
      "  <unk>: 0.622\n",
      "  <mask>: 0.400\n",
      "  D3.F#3.A3.C4_s: 0.099\n",
      "  B4.E5.G#5.E5_s: 0.099\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 2.545\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.758\n",
      "  <unk>: 2.074\n",
      "  A1.B1_s: 0.593\n",
      "  A1.B1.C#2_s: 0.593\n",
      "  A1.A2_2/3: 0.593\n",
      "\n",
      "DEBUG LOGITS at position (0,32):\n",
      "Token 42199 original logit: 2.062\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 3.041\n",
      "  <unk>: 0.149\n",
      "  A3.E3.C#3.A2_0.75: -0.771\n",
      "  A3.E-5_e: -0.771\n",
      "  A3.D3.A2_q: -0.771\n",
      "\n",
      "DEBUG LOGITS at position (0,24):\n",
      "Token 42199 original logit: 1.990\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.676\n",
      "  <unk>: 1.204\n",
      "  <mask>: 0.263\n",
      "  G#4.F4.D4.B3_q: -0.343\n",
      "  F#4.A4.D5_1.5: -0.343\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 1.811\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 1.248\n",
      "  <mask>: -0.097\n",
      "  A2.A1.A1.A2_1/3: -1.261\n",
      "  A1.C5_1/3: -1.261\n",
      "  A1.C2.E-2_9.0: -1.261\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 5.125\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.941\n",
      "  <unk>: 0.542\n",
      "  A2.C#4_0.75: -1.163\n",
      "  A1_h: -1.163\n",
      "  A1_1/6: -1.163\n",
      "\n",
      "DEBUG LOGITS at position (0,34):\n",
      "Token 42199 original logit: -1.423\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 2.508\n",
      "  <TRACKS>: -1.071\n",
      "  <|startofpiece|>: -1.071\n",
      "  <TRACKSEP>: -1.071\n",
      "  <|endofpiece|>: -1.071\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 2.572\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.250\n",
      "  <unk>: -0.212\n",
      "  F5.G#4_1/3: -0.595\n",
      "  A1.A2_1.25: -0.596\n",
      "  A1.A2_0.75: -0.596\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 3.637\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 0.591\n",
      "  <unk>: -0.176\n",
      "  C6.D6.C6.D6_s: -0.499\n",
      "  F#5.A5.C6.E-6_1/12: -0.499\n",
      "  A1.A2_s: -0.499\n",
      "\n",
      "DEBUG LOGITS at position (0,25):\n",
      "Token 42199 original logit: 8.094\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.879\n",
      "  <unk>: 1.043\n",
      "  A6_3.75: 0.274\n",
      "  A5.A4.D4.D3_1/3: 0.274\n",
      "  A3.C#3.E3_s: 0.274\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 1.878\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.973\n",
      "  <unk>: 1.539\n",
      "  B-3.B-4_5/12: -0.225\n",
      "  A3.G#4.G#5_1/3: -0.225\n",
      "  A3.F4.C5_q: -0.225\n",
      "\n",
      "DEBUG LOGITS at position (0,9):\n",
      "Token 42199 original logit: 4.465\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 4.617\n",
      "  F4.A4.D5_s: -1.813\n",
      "  A5.F#4.D5.A4.D4.A5.D5.F#4.D4.A4.D5.F#4.A4.D4_s: -1.813\n",
      "  G#4.F4.D4.B3_q: -1.813\n",
      "  E-4.C4.A4_1.25: -1.813\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 1.680\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.621\n",
      "  <unk>: -0.280\n",
      "  <mask>: -1.039\n",
      "  <|endofpiece|>: -1.120\n",
      "  <|startofpiece|>: -1.120\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 4.027\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 4.906\n",
      "  F4.C5.A5_e: 0.022\n",
      "  F4.G4.A4_1/3: 0.022\n",
      "  F#5.A5.C6.E-6_1/12: 0.022\n",
      "  B-4.C#5_q: 0.021\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: -1.768\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 7.191\n",
      "  <unk>: -1.779\n",
      "  <TRACKS>: -1.803\n",
      "  <|endofpiece|>: -1.803\n",
      "  <|startofpiece|>: -1.803\n",
      "\n",
      "DEBUG LOGITS at position (0,25):\n",
      "Token 42199 original logit: -0.339\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 3.584\n",
      "  A1.C2.E-2_9.0: -1.536\n",
      "  A1.A2_0.75: -1.536\n",
      "  A1_e: -1.536\n",
      "  A1.A2_1/12: -1.536\n",
      "\n",
      "DEBUG LOGITS at position (0,9):\n",
      "Token 42199 original logit: 4.430\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 10.711\n",
      "  B5.D6_0.75: -1.493\n",
      "  <TRACKS>: -1.494\n",
      "  <|endofpiece|>: -1.494\n",
      "  <|startofpiece|>: -1.494\n",
      "\n",
      "DEBUG LOGITS at position (0,9):\n",
      "Token 42199 original logit: 1.681\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 5.086\n",
      "  A3.D3.F#3_s: -1.558\n",
      "  A3.C4.F#4.E-4_s: -1.558\n",
      "  A4.C#5_q: -1.558\n",
      "  A3.C4.F#4_s: -1.558\n",
      "\n",
      "DEBUG LOGITS at position (0,27):\n",
      "Token 42199 original logit: 1.872\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 6.895\n",
      "  <unk>: 2.691\n",
      "  <mask>: 1.090\n",
      "  A2.A1_2/3: 0.435\n",
      "  A1.F#1_s: 0.435\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.125\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 4.992\n",
      "  <unk>: 1.860\n",
      "  A1.A2_2.5: 0.783\n",
      "  A1.A2_1.5: 0.783\n",
      "  <|startofpiece|>: 0.783\n",
      "\n",
      "DEBUG LOGITS at position (0,26):\n",
      "Token 42199 original logit: 3.184\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 2.477\n",
      "  B3.G4.E5_0.75: -0.806\n",
      "  B-2.B-3_1.5: -0.806\n",
      "  C#5.B3.E4.B4_s: -0.806\n",
      "  B-4.B-5.D5_s: -0.806\n",
      "\n",
      "DEBUG LOGITS at position (0,32):\n",
      "Token 42199 original logit: -0.062\n",
      "Token 42199 masked logit: -10000000000.000\n",
      "Token 42199 allowed: False\n",
      "Top 5 after masking:\n",
      "  </s>: 1.051\n",
      "  A3.E3_e: -0.773\n",
      "  A3.A2.E5_s: -0.773\n",
      "  A3.G#4.G#5_1/3: -0.773\n",
      "  A3.D3.A2_q: -0.773\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mretrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_lora\n\u001b[1;32m----> 3\u001b[0m metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Codess & Projects\\Project Mozart\\[â³] Model 1\\5) Training\\retrain.py:431\u001b[0m, in \u001b[0;36mtrain_lora\u001b[1;34m(model, tokenizer)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 431\u001b[0m metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_history\n",
      "File \u001b[1;32md:\\Codess & Projects\\Project Mozart\\[â³] Model 1\\5) Training\\retrain.py:192\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, tokenizer, tokenized)\u001b[0m\n\u001b[0;32m    189\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39mUSE_FP16, device_type\u001b[38;5;241m=\u001b[39mDEVICE):\n\u001b[1;32m--> 192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# --------------------------------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Only debug first step\u001b[39;00m\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\peft\\peft_model.py:2181\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, decoder_input_ids, decoder_attention_mask, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   2179\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2180\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 2181\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   2182\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2183\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   2184\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   2185\u001b[0m             decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m   2186\u001b[0m             decoder_attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m   2187\u001b[0m             decoder_inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m   2188\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   2189\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2190\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2191\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   2192\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2193\u001b[0m         )\n\u001b[0;32m   2195\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2197\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1472\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1468\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1469\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1470\u001b[0m         )\n\u001b[1;32m-> 1472\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1491\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1492\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1271\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1268\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1271\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:872\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    870\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 872\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:327\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states))\n\u001b[0;32m    326\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m--> 327\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Codess & Projects\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from retrain import train_lora\n",
    "\n",
    "metrics_history = train_lora(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede64c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
