{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d35a96a",
   "metadata": {},
   "source": [
    "# 1) Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e691f61",
   "metadata": {},
   "source": [
    "## Tokenize MIDI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb366e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing MIDI files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [00:16<00:00, 11.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from miditok import REMI\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Choose a tokenizer\n",
    "tokenizer = REMI()\n",
    "\n",
    "# Path to MIDI dataset\n",
    "MIDI_DIR = Path(\"../data/audio\")\n",
    "\n",
    "# Enocde all MIDI files into tokens\n",
    "tokenized = []\n",
    "\n",
    "for midi_path in tqdm(list(MIDI_DIR.glob(\"*.mid\")), desc=\"Tokenizing MIDI files\"):\n",
    "    tokens = tokenizer.encode(midi_path) # pass path directly\n",
    "    tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bbbd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to tokenizer/\n"
     ]
    }
   ],
   "source": [
    "# Save tokenizer\n",
    "SAVE_DIR = Path(\"tokenizer\")\n",
    "\n",
    "# Save\n",
    "tokenizer.save(SAVE_DIR)\n",
    "\n",
    "print(f\"Tokenizer saved to {SAVE_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d556592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer reloaded!\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer (testing)\n",
    "LOAD_DIR = Path(\"tokenizer/tokenizer.json\")\n",
    "\n",
    "tokenizer2 = REMI(params=LOAD_DIR)\n",
    "\n",
    "print(\"Tokenizer reloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43bbbea",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94071ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 276\n",
      "First sequence: [4, 190, 56, 108, 128, 32, 105, 127, 196, 56, 108, 127, 44, 102, 126, 202, 63, 115, 127, 48]\n"
     ]
    }
   ],
   "source": [
    "def to_int_list(seq):\n",
    "    \"\"\"\n",
    "    Convert any seq (TokSequence, list of TokSequence, or list of ints) \n",
    "    into a flat list of ints.\n",
    "    \"\"\"\n",
    "    if hasattr(seq, \"ids\"):  # direct TokSequence\n",
    "        return list(seq.ids)\n",
    "    \n",
    "    elif isinstance(seq, (list, tuple)):\n",
    "        out = []\n",
    "        for x in seq:\n",
    "            if hasattr(x, \"ids\"):      # nested TokSequence\n",
    "                out.extend(list(x.ids))\n",
    "            elif isinstance(x, int):\n",
    "                out.append(x)\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected element type: {type(x)}\")\n",
    "        return out\n",
    "    \n",
    "    elif isinstance(seq, int):\n",
    "        return [seq]\n",
    "    \n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected type at top level: {type(seq)}\")\n",
    "\n",
    "\n",
    "# Apply to all your tokenized data\n",
    "int_seqs = [to_int_list(seq) for seq in tokenized]\n",
    "\n",
    "# Compute vocab size\n",
    "vocab_size = max(max(seq) for seq in int_seqs) + 1\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(\"First sequence:\", int_seqs[0][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adc08063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "    def __init__(self, sequences, seq_len=32, pad_id=0, stride=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: list of token ID lists\n",
    "            seq_len: length of each training sequence\n",
    "            pad_id: padding token ID\n",
    "            stride: step size for sliding window\n",
    "                    - 1 = fully overlapping\n",
    "                    - seq_len = non-overlapping\n",
    "                    - e.g. 8 = partial overlap\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.seq_len = seq_len\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        for seq in tqdm(sequences, desc=\"Creating dataset\"):\n",
    "            ids = list(seq)\n",
    "\n",
    "            if len(ids) < 2:  # skip too-short sequences\n",
    "                continue\n",
    "\n",
    "            # Sliding windows with configurable stride\n",
    "            for i in range(0, len(ids) - 1, stride):\n",
    "                x = ids[i:i+seq_len]\n",
    "                y = ids[i+1:i+seq_len+1]\n",
    "\n",
    "                # Pad if shorter than seq_len\n",
    "                if len(x) < seq_len:\n",
    "                    x = pad(torch.tensor(x, dtype=torch.long),\n",
    "                            (0, seq_len - len(x)), value=pad_id)\n",
    "                    y = pad(torch.tensor(y, dtype=torch.long),\n",
    "                            (0, seq_len - len(y)), value=pad_id)\n",
    "                else:\n",
    "                    x = torch.tensor(x, dtype=torch.long)\n",
    "                    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "                self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d2c460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_id=0):\n",
    "    \"\"\"Pads a batch of variable-length sequences to the max length in batch.\"\"\"\n",
    "    xs, ys = zip(*batch)  # unzip\n",
    "    xs = pad_sequence(xs, batch_first=True, padding_value=pad_id)\n",
    "    ys = pad_sequence(ys, batch_first=True, padding_value=pad_id)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55e5044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 182/182 [00:07<00:00, 23.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 488913\n",
      "x shape: torch.Size([32, 32])\n",
      "y shape: torch.Size([32, 32])\n",
      "x[0]: tensor([112, 129, 206,  65, 112, 137, 218,  68, 112, 129,   4, 190,  65, 112,\n",
      "        129, 194,  63, 112, 129, 198,  62, 112, 129, 202,  63, 112, 129, 206,\n",
      "         56, 112, 126,  68])\n",
      "y[0]: tensor([129, 206,  65, 112, 137, 218,  68, 112, 129,   4, 190,  65, 112, 129,\n",
      "        194,  63, 112, 129, 198,  62, 112, 129, 202,  63, 112, 129, 206,  56,\n",
      "        112, 126,  68, 112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = MIDIDataset(int_seqs, seq_len=32, pad_id=0, stride=8)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True,\n",
    "                    collate_fn=lambda b: collate_fn(b, pad_id=0))\n",
    "\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "# Grab a single batch\n",
    "x, y = next(iter(loader))\n",
    "\n",
    "print(\"x shape:\", x.shape)  # (batch_size, max_len_in_batch)\n",
    "print(\"y shape:\", y.shape)  # (batch_size, max_len_in_batch)\n",
    "print(\"x[0]:\", x[0])        # first input sequence\n",
    "print(\"y[0]:\", y[0])        # corresponding target sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649033e7",
   "metadata": {},
   "source": [
    "# 2) Create Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d57ab1",
   "metadata": {},
   "source": [
    "## Define a Simple Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f6f81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Transformer model for music token prediction.\n",
    "    Takes a sequence of token IDs (from MIDI tokenization)\n",
    "    and predicts the next token in the sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                vocab_size: int,   # number of unique tokens (notes, events, etc.)\n",
    "                embed_dim: int = 256, \n",
    "                n_heads: int = 4, \n",
    "                n_layers: int = 4,\n",
    "                ff_dim: int = 512, # feedforward dimension inside Transformer\n",
    "                dropout: float = 0.1,\n",
    "                max_seq_len: int = 1024):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional encoding (learnable, not sinusoidal)\n",
    "        self.positional_encoding = nn.Parameter(\n",
    "            torch.zeros(1, max_seq_len, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder (batch_first=True removes permutes!)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True   # keep batch dim first\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # Final projection to vocab\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len) of token IDs\n",
    "        Returns:\n",
    "            (batch, seq_len, vocab_size) of logits\n",
    "        \"\"\"\n",
    "        # Embed tokens: (batch, seq_len, embed_dim)\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # Run through Transformer\n",
    "        out = self.transformer(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Project to vocab\n",
    "        return self.fc(out)  # (batch, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423b905",
   "metadata": {},
   "source": [
    "# 3) Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d80c2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/15279 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15279/15279 [05:52<00:00, 43.40it/s, step=15279/15279, loss=0.0840, avg_loss=0.6239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished | Average Loss: 0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15279/15279 [05:51<00:00, 43.41it/s, step=15279/15279, loss=0.0901, avg_loss=0.0942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished | Average Loss: 0.0942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15279/15279 [05:41<00:00, 44.78it/s, step=15279/15279, loss=0.0646, avg_loss=0.0862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished | Average Loss: 0.0862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15279/15279 [05:53<00:00, 43.27it/s, step=15279/15279, loss=0.0838, avg_loss=0.0823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished | Average Loss: 0.0823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15279/15279 [05:57<00:00, 42.77it/s, step=15279/15279, loss=0.0596, avg_loss=0.0794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished | Average Loss: 0.0794\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 5\n",
    "\n",
    "# Initialize model, optimizer, and loss\n",
    "model = MusicTransformer(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(loader), \n",
    "                        desc=f\"Epoch {epoch+1}/{EPOCHS}\", \n",
    "                        total=len(loader),\n",
    "                        leave=True)\n",
    "\n",
    "    for step, (x, y) in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass (shift inputs/targets)\n",
    "        logits = model(x[:, :-1])\n",
    "        loss = loss_fn(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            y[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (step + 1)\n",
    "\n",
    "        # Update progress bar live\n",
    "        progress_bar.set_postfix({\n",
    "            \"step\": f\"{step+1}/{len(loader)}\",\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"avg_loss\": f\"{avg_loss:.4f}\"\n",
    "        })\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished | Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "    }, f\"checkpoint_epoch{epoch+1}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47839ba3",
   "metadata": {},
   "source": [
    "# 4) Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cda806",
   "metadata": {},
   "source": [
    "## vocab utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "992a1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note name -> MIDI number (C4 = 60)\n",
    "NOTE_TO_SEMITONE = {\n",
    "    'C': 0, 'C#': 1, 'Db': 1, 'D': 2, 'D#': 3, 'Eb': 3, 'E': 4, 'F': 5,\n",
    "    'F#': 6, 'Gb': 6, 'G': 7, 'G#': 8, 'Ab': 8, 'A': 9, 'A#': 10, 'Bb': 10, 'B': 11\n",
    "}\n",
    "\n",
    "\n",
    "def build_vocab_maps(tokenizer):\n",
    "    # tokenizer.vocab is a dict token->id as you showed\n",
    "    vocab = tokenizer.vocab\n",
    "    id_to_token = {int(v): k for k, v in vocab.items()}\n",
    "    return vocab, id_to_token\n",
    "\n",
    "\n",
    "def find_token_key_starting_with(vocab, prefix):\n",
    "    # returns token key matching prefix exactly or starting with prefix\n",
    "    if prefix in vocab:\n",
    "        return prefix\n",
    "    # if multiple tokens share prefix (e.g. Duration_1.0.8) pick the first found\n",
    "    for k in vocab:\n",
    "        if k.startswith(prefix):\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "\n",
    "def choose_velocity_token(vocab, target_vel=90):\n",
    "    # collect velocity numeric tokens\n",
    "    vel_tokens = []\n",
    "    for k in vocab:\n",
    "        if k.startswith(\"Velocity_\"):\n",
    "            try:\n",
    "                n = int(k.split(\"_\", 1)[1])\n",
    "                vel_tokens.append((n, k))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not vel_tokens:\n",
    "        return None\n",
    "    # pick token with closest numeric velocity to target\n",
    "    vel_tokens.sort(key=lambda x: abs(x[0] - target_vel))\n",
    "    return vel_tokens[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237f633",
   "metadata": {},
   "source": [
    "## note utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97f8ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_name_to_midi(note):\n",
    "    # Accept formats like C4, C#4, Db4\n",
    "    # Split letter(s) and octave\n",
    "    import re\n",
    "    m = re.match(r'^([A-G][#b]?)(-?\\d+)$', note)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Invalid note name: {note}\")\n",
    "    name, octave = m.group(1), int(m.group(2))\n",
    "    return 12 * (octave + 1) + NOTE_TO_SEMITONE[name]  # MIDI formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2609f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_token_ids_from_symbolic(tokenizer, note_symbol, default_velocity=90):\n",
    "    \"\"\"\n",
    "    Convert a single symbolic like \"C4_q\" -> list of token ids [Pitch_x, Velocity_y, Duration_z]\n",
    "    duration codes supported: w,h,q,e (whole, half, quarter, eighth)\n",
    "    \"\"\"\n",
    "    vocab, _ = build_vocab_maps(tokenizer)\n",
    "    # parse like \"C4_q\" or \"C#4_q\"\n",
    "    if \"_\" not in note_symbol:\n",
    "        raise ValueError(\"Symbolic note must be like 'C4_q'\")\n",
    "    note_part, dur_code = note_symbol.split(\"_\", 1)\n",
    "\n",
    "    midi = note_name_to_midi(note_part)   # 60 for C4\n",
    "    pitch_key = f\"Pitch_{midi}\"\n",
    "    pitch_token = find_token_key_starting_with(vocab, pitch_key)\n",
    "    if pitch_token is None:\n",
    "        raise RuntimeError(f\"Pitch token for MIDI {midi} not found in vocab\")\n",
    "\n",
    "    # duration mapping (assumes vocab durations use numeric beats like 1.0, 0.5, 2.0, ...)\n",
    "    dur_map = {\n",
    "        'w': '4.0',   # whole\n",
    "        'h': '2.0',   # half\n",
    "        'q': '1.0',   # quarter\n",
    "        'e': '0.5',   # eighth\n",
    "        's': '0.25',  # sixteenth (if available)\n",
    "    }\n",
    "    dur_value = dur_map.get(dur_code, None)\n",
    "    if dur_value is None:\n",
    "        # maybe user passed exact duration like \"1.0.8\"\n",
    "        dur_value = dur_code\n",
    "\n",
    "    duration_prefix = f\"Duration_{dur_value}\"\n",
    "    duration_token = find_token_key_starting_with(vocab, duration_prefix)\n",
    "    if duration_token is None:\n",
    "        # fallback: pick a default duration token (quarter-ish)\n",
    "        duration_token = find_token_key_starting_with(vocab, \"Duration_1.0\")\n",
    "        if duration_token is None:\n",
    "            raise RuntimeError(\"No suitable Duration token found in vocab\")\n",
    "\n",
    "    velocity_token = choose_velocity_token(vocab, target_vel=default_velocity)\n",
    "    if velocity_token is None:\n",
    "        # fallback: try a specific velocity token name\n",
    "        velocity_token = find_token_key_starting_with(vocab, \"Velocity_95\")\n",
    "        if velocity_token is None:\n",
    "            raise RuntimeError(\"No Velocity token found in vocab\")\n",
    "\n",
    "    # return ids as integers\n",
    "    return [int(vocab[pitch_token]), int(vocab[velocity_token]), int(vocab[duration_token])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae90a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DUR_CODES = [\"w\", \"h\", \"q\", \"e\", \"s\"]  # whole, half, quarter, eighth, sixteenth\n",
    "\n",
    "def random_note_symbol(octave_range=(3,5)):\n",
    "    \"\"\"\n",
    "    Generate a random symbolic note like 'C4_q' or 'F#5_e'\n",
    "    \"\"\"\n",
    "    note_names = [\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"A\",\"A#\",\"B\"]\n",
    "    note = random.choice(note_names)\n",
    "    octave = random.randint(octave_range[0], octave_range[1])\n",
    "    dur = random.choice(DUR_CODES)\n",
    "    return f\"{note}{octave}_{dur}\"\n",
    "\n",
    "def random_start_symbols(n=3, octave_range=(3,5)):\n",
    "    \"\"\"\n",
    "    Generate N random symbolic notes\n",
    "    \"\"\"\n",
    "    return [random_note_symbol(octave_range) for _ in range(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f3003",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a5d77827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the type of token\n",
    "def token_type(token_name):\n",
    "    for t in [\"Pitch\", \"Velocity\", \"Duration\", \"Bar\", \"Position\"]:\n",
    "        if token_name.startswith(t):\n",
    "            return t\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0fc3f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from miditok import TokSequence\n",
    "\n",
    "def generate_with_rhythm(\n",
    "    model, tokenizer,\n",
    "    start_symbols=[\"C4_q\", \"E4_q\", \"G4_q\"],\n",
    "    max_len=2000,\n",
    "    out_path=\"model_outputs/generated_rhythm.mid\",\n",
    "    max_input_len_window=1024,\n",
    "    default_velocity=90,\n",
    "    stop_on_eos=True\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start symbols to token ids\n",
    "    start_ids = []\n",
    "    for sym in start_symbols:\n",
    "        start_ids.extend(note_token_ids_from_symbolic(tokenizer, sym, default_velocity))\n",
    "\n",
    "    ids = start_ids.copy()\n",
    "    vocab, id_to_token = build_vocab_maps(tokenizer)\n",
    "\n",
    "    # Try to find EOS id\n",
    "    eos_id = int(vocab[\"EOS_None\"]) if \"EOS_None\" in vocab else None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_len):\n",
    "            window = ids[-max_input_len_window:]\n",
    "            x = torch.tensor([window], dtype=torch.long, device=device)\n",
    "            out = model(x)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "            if not isinstance(logits, torch.Tensor):\n",
    "                logits = torch.tensor(logits, device=device)\n",
    "\n",
    "            next_id = int(torch.argmax(logits[0, -1]).item())\n",
    "            token_name = id_to_token.get(next_id, \"\")\n",
    "\n",
    "            # --- Smarter Rhythm Filtering ---\n",
    "            # Get last non-Bar/Position token to determine expected note type\n",
    "            last_note_tokens = [id_to_token.get(t, \"\") for t in reversed(ids) \n",
    "                                if not id_to_token.get(t, \"\").startswith((\"Bar\", \"Position\"))]\n",
    "            last_type = token_type(last_note_tokens[0]) if last_note_tokens else None\n",
    "\n",
    "            # Determine expected type for note triplet\n",
    "            if last_type == \"Pitch\":\n",
    "                expected = \"Velocity\"\n",
    "            elif last_type == \"Velocity\":\n",
    "                expected = \"Duration\"\n",
    "            else:\n",
    "                expected = \"Pitch\"  # start of new triplet\n",
    "\n",
    "            # Always accept Bar/Position tokens; otherwise enforce triplet order\n",
    "            if token_name.startswith((\"Bar\", \"Position\")) or token_name.startswith(expected):\n",
    "                ids.append(next_id)\n",
    "\n",
    "            if stop_on_eos and eos_id is not None and next_id == eos_id:\n",
    "                print(f\"Hit EOS at step {step+1}\")\n",
    "                break\n",
    "\n",
    "            if (step + 1) % 500 == 0:\n",
    "                print(f\"Generated {step+1} tokens (current length {len(ids)})...\")\n",
    "\n",
    "    # Decode with TokSequence\n",
    "    seq = TokSequence(ids=ids)\n",
    "    decoded = tokenizer.decode([seq])\n",
    "    midi_obj = decoded[0] if isinstance(decoded, (list, tuple)) else decoded\n",
    "\n",
    "    # Save MIDI\n",
    "    if hasattr(midi_obj, \"dump_midi\"):\n",
    "        midi_obj.dump_midi(out_path)\n",
    "    elif hasattr(midi_obj, \"dumps_midi\"):\n",
    "        data = midi_obj.dumps_midi()\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(data if isinstance(data, bytes) else data.encode())\n",
    "    else:\n",
    "        raise RuntimeError(\"Decoded object has no known dump method\")\n",
    "\n",
    "    print(f\"âœ… Saved MIDI to {out_path}\")\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2db277d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ Random start notes: ['F#5_h', 'B4_q', 'F3_h']\n",
      "Generated 500 tokens (current length 9)...\n",
      "Generated 1000 tokens (current length 9)...\n",
      "Generated 1500 tokens (current length 9)...\n",
      "Generated 2000 tokens (current length 9)...\n",
      "Generated 2500 tokens (current length 9)...\n",
      "Generated 3000 tokens (current length 9)...\n",
      "Generated 3500 tokens (current length 9)...\n",
      "Generated 4000 tokens (current length 9)...\n",
      "Generated 4500 tokens (current length 9)...\n",
      "Generated 5000 tokens (current length 9)...\n",
      "âœ… Saved MIDI to model_outputs/generated_rhythm.mid\n"
     ]
    }
   ],
   "source": [
    "# OR use random start notes\n",
    "rand_syms = random_start_symbols(n=3)\n",
    "print(\"ðŸŽµ Random start notes:\", rand_syms)\n",
    "ids = generate_with_rhythm(model, tokenizer, start_symbols=rand_syms, max_len=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c321a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def midi_to_wav(midi_path, \n",
    "                soundfont_path=\"../soundfonts/AegeanSymphonicOrchestra-SND.sf2\", \n",
    "                output_dir=\"model_outputs/\", \n",
    "                output_name=None, \n",
    "                sample_rate=44100, \n",
    "                gain=2.0,\n",
    "                normalize=True,\n",
    "                fluidsynth_path=r\"C:\\tools\\fluidsynth\\bin\\fluidsynth.exe\",\n",
    "                print_details=True):\n",
    "    \"\"\"\n",
    "    Convert a MIDI file to WAV using FluidSynth and a specified SoundFont.\n",
    "\n",
    "    Args:\n",
    "        midi_path (str): Path to the input MIDI file.\n",
    "        soundfont_path (str): Path to the SoundFont (.sf2) file.\n",
    "        output_dir (str): Directory to save the output WAV.\n",
    "        output_name (str): Optional output filename (without extension). Defaults to MIDI basename.\n",
    "        sample_rate (int): Sampling rate for WAV.\n",
    "        gain (float): Gain multiplier for FluidSynth.\n",
    "        normalize (bool): Normalize the audio using pydub.\n",
    "        fluidsynth_path (str): Path to FluidSynth executable.\n",
    "        print_details (bool): Whether to print logs.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the generated WAV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Determine output filename\n",
    "    if output_name is None:\n",
    "        base = os.path.splitext(os.path.basename(midi_path))[0]\n",
    "        output_name = base\n",
    "    output_wav = os.path.join(output_dir, f\"{output_name}.wav\")\n",
    "\n",
    "    if print_details:\n",
    "        print(f\"Rendering '{midi_path}' to WAV...\")\n",
    "        print(f\"Using SoundFont: {soundfont_path}\")\n",
    "        print(f\"Output path: {output_wav}\")\n",
    "\n",
    "    # FluidSynth command\n",
    "    cmd = [\n",
    "        fluidsynth_path,\n",
    "        \"-a\", \"file\",          # write to file\n",
    "        \"-F\", output_wav,      # output WAV\n",
    "        \"-ni\", soundfont_path, # load SoundFont\n",
    "        \"-r\", str(sample_rate),\n",
    "        \"-g\", str(gain),\n",
    "        midi_path\n",
    "    ]\n",
    "\n",
    "    # Run FluidSynth\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running FluidSynth: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Normalize audio if requested\n",
    "    if normalize:\n",
    "        sound = AudioSegment.from_wav(output_wav)\n",
    "        normalized = sound.normalize()\n",
    "        normalized.export(output_wav, format=\"wav\")\n",
    "\n",
    "    if print_details:\n",
    "        print(f\"âœ… Successfully converted MIDI to WAV: {output_wav}\")\n",
    "\n",
    "    return output_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f95669b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering 'model_outputs/demo_generated_long.mid' to WAV...\n",
      "Using SoundFont: ../soundfonts/AegeanSymphonicOrchestra-SND.sf2\n",
      "Output path: model_outputs/demo_generated_long.wav\n",
      "âœ… Successfully converted MIDI to WAV: model_outputs/demo_generated_long.wav\n"
     ]
    }
   ],
   "source": [
    "midi_path = \"model_outputs/demo_generated_long.mid\"\n",
    "wav_path = midi_to_wav(midi_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b879fe",
   "metadata": {},
   "source": [
    "# DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "91fa1d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0 (first 50 tokens):\n",
      "[4, 190, 56, 108, 128, 32, 105, 127, 196, 56, 108, 127, 44, 102, 126, 202, 63, 115, 127, 48, 106, 126, 207, 63, 114, 127, 44, 105, 126, 213, 65, 112, 127, 49, 104, 126, 218, 65, 108, 127, 44, 103, 126, 4, 192, 63, 108, 126, 48, 100]\n",
      "Sequence 1 (first 50 tokens):\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 190, 61, 107, 139, 206, 60, 107, 129, 210, 61, 106, 129, 214, 60, 107, 129, 218, 61, 106, 129, 4, 190, 63, 107, 129, 194, 61, 106, 129, 198, 59, 106, 129, 202, 58, 106, 129, 206, 58, 107]\n",
      "Sequence 2 (first 50 tokens):\n",
      "[4, 4, 190, 65, 116, 149, 214, 63, 116, 133, 4, 190, 62, 116, 132, 198, 63, 117, 137, 210, 65, 116, 129, 214, 66, 116, 129, 218, 68, 116, 129, 4, 190, 69, 113, 132, 4, 190, 63, 116, 149, 214, 61, 116, 133, 4, 190, 60, 116, 133]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first 3 sequences\n",
    "for i, seq in enumerate(int_seqs[:3]):\n",
    "    print(f\"Sequence {i} (first 50 tokens):\")\n",
    "    print(seq[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c27248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar_None, Bar_None, Position_0, Pitch_81, Velocity_91, Duration_3.0.8, Position_24, Pitch_79, Velocity_91, Duration_1.0.8, Bar_None, Position_0, Pitch_78, Velocity_91, Duration_0.7.8, Position_8, Pitch_79, Velocity_95, Duration_1.4.8, Position_20, Pitch_81, Velocity_91, Duration_0.4.8, Position_24, Pitch_82, Velocity_91, Duration_0.4.8, Position_28, Pitch_84, Velocity_91, Duration_0.4.8, Bar_None, Position_0, Pitch_85, Velocity_79, Duration_0.7.8, Bar_None, Position_0, Pitch_79, Velocity_91, Duration_3.0.8, Position_24, Pitch_77, Velocity_91, Duration_1.0.8, Bar_None, Position_0, Pitch_76, Velocity_91, Duration_1.0.8, \n"
     ]
    }
   ],
   "source": [
    "vocab, id_to_token = build_vocab_maps(tokenizer)\n",
    "for t in seq[:50]:\n",
    "    print(id_to_token[t], end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a274c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ Random start notes: ['B3_q', 'D#5_e', 'B4_s']\n",
      "Generated 500 tokens (current length 509)...\n",
      "Generated 1000 tokens (current length 1009)...\n",
      "Generated 1500 tokens (current length 1509)...\n",
      "Generated 2000 tokens (current length 2009)...\n",
      "âœ… Saved raw MIDI to model_outputs/raw_output.mid\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from miditok import TokSequence\n",
    "\n",
    "def generate_raw(model, tokenizer, start_symbols=[\"C4_q\", \"E4_q\", \"G4_q\"], max_len=2000, out_path=\"model_outputs/raw_output.mid\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Convert start symbols to token IDs\n",
    "    start_ids = []\n",
    "    for sym in start_symbols:\n",
    "        start_ids.extend(note_token_ids_from_symbolic(tokenizer, sym))\n",
    "\n",
    "    ids = start_ids.copy()\n",
    "    vocab, id_to_token = build_vocab_maps(tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_len):\n",
    "            x = torch.tensor([ids[-1024:]], dtype=torch.long, device=device)\n",
    "            out = model(x)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            next_id = int(torch.argmax(logits[0, -1]).item())\n",
    "            ids.append(next_id)\n",
    "\n",
    "            if (step + 1) % 500 == 0:\n",
    "                print(f\"Generated {step+1} tokens (current length {len(ids)})...\")\n",
    "\n",
    "    # Map IDs back to token names\n",
    "    token_names = [id_to_token.get(i, str(i)) for i in ids]\n",
    "\n",
    "    # Decode to MIDI (without filtering)\n",
    "    seq = TokSequence(ids=ids)\n",
    "    try:\n",
    "        decoded = tokenizer.decode([seq])\n",
    "    except Exception:\n",
    "        decoded = tokenizer(seq)\n",
    "\n",
    "    midi_obj = decoded[0] if isinstance(decoded, (list, tuple)) else decoded\n",
    "    if hasattr(midi_obj, \"dump_midi\"):\n",
    "        midi_obj.dump_midi(out_path)\n",
    "    elif hasattr(midi_obj, \"dumps_midi\"):\n",
    "        data = midi_obj.dumps_midi()\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            if isinstance(data, str):\n",
    "                f.write(data.encode())\n",
    "            else:\n",
    "                f.write(data)\n",
    "\n",
    "    print(f\"âœ… Saved raw MIDI to {out_path}\")\n",
    "    return ids, token_names\n",
    "\n",
    "# Example usage\n",
    "rand_syms = random_start_symbols(n=3)\n",
    "print(\"ðŸŽµ Random start notes:\", rand_syms)\n",
    "ids, tokens = generate_raw(model, tokenizer, start_symbols=rand_syms, max_len=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779dd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
